{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 721 ms, sys: 60 ms, total: 781 ms\n",
      "Wall time: 806 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup, BertForSequenceClassification\n",
    "\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncolumn_names = [\\'content\\', \\'label\\', \\'hotel_name\\']\\ndf = pd.DataFrame(columns = column_names)\\n\\nroot_dir = \"./Crawler/CrawlerToBooking/\"\\ndirectories = [\\'negativeReviews\\', \\'positiveReviews\\']\\n\\nfor label, directory in enumerate(directories):\\n    txt_path = os.path.join(root_dir, directory)\\n    for filename in os.listdir(txt_path):\\n        hotel_name = os.path.splitext(filename)[0]\\n        with open(os.path.join(txt_path, filename)) as f:\\n            content = f.read()\\n            dict_new = {\"content\":content.strip().replace(\"\\n\",\"\"), \"label\":str(label), \"hotel_name\":hotel_name}\\n            df = df.append(dict_new, ignore_index=True)\\ndf.to_csv(\"reviews.csv\", index=False)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "column_names = ['content', 'label', 'hotel_name']\n",
    "df = pd.DataFrame(columns = column_names)\n",
    "\n",
    "root_dir = \"./Crawler/CrawlerToBooking/\"\n",
    "directories = ['negativeReviews', 'positiveReviews']\n",
    "\n",
    "for label, directory in enumerate(directories):\n",
    "    txt_path = os.path.join(root_dir, directory)\n",
    "    for filename in os.listdir(txt_path):\n",
    "        hotel_name = os.path.splitext(filename)[0]\n",
    "        with open(os.path.join(txt_path, filename)) as f:\n",
    "            content = f.read()\n",
    "            dict_new = {\"content\":content.strip().replace(\"\\n\",\"\"), \"label\":str(label), \"hotel_name\":hotel_name}\n",
    "            df = df.append(dict_new, ignore_index=True)\n",
    "df.to_csv(\"reviews.csv\", index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>hotel_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64160</th>\n",
       "      <td>早餐人太多排隊排很久</td>\n",
       "      <td>0</td>\n",
       "      <td>娜路彎大酒店329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19452</th>\n",
       "      <td>無</td>\n",
       "      <td>0</td>\n",
       "      <td>花蓮鳥窩青年旅舍3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266740</th>\n",
       "      <td>很舒適的旅館和服務，下次會再來...早餐好吃極了...</td>\n",
       "      <td>1</td>\n",
       "      <td>東驛商旅 西寧館11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126829</th>\n",
       "      <td>早餐有點差</td>\n",
       "      <td>0</td>\n",
       "      <td>雙星大飯店1508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129917</th>\n",
       "      <td>地點有點尷尬，周遭的華陰街晚上基本上都大門深鎖沒有宵夜，因此不如南陽街；也由於鄰近店家很早就...</td>\n",
       "      <td>0</td>\n",
       "      <td>龍蝦先生的秘密巢穴設計青旅7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122265</th>\n",
       "      <td>沒有窗戶</td>\n",
       "      <td>0</td>\n",
       "      <td>台北大亞帝國H709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276984</th>\n",
       "      <td>房間乾淨舒適，cp值高！值得推薦！</td>\n",
       "      <td>1</td>\n",
       "      <td>新京站民宿63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91502</th>\n",
       "      <td>房間打掃不完整 桌上迎賓糖果裡居然有已經吃過的糖果紙包裝</td>\n",
       "      <td>0</td>\n",
       "      <td>昭盛52行館18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240231</th>\n",
       "      <td>床很舒服 床間也很棒行李部的服務員也很好 很樂於助人</td>\n",
       "      <td>1</td>\n",
       "      <td>義大皇家酒店55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258390</th>\n",
       "      <td>早餐還有吐司、貝果，水果切盤，飲料咖啡無限喝，床很舒適，吧檯員工新來的很風趣、很健談、重點會...</td>\n",
       "      <td>1</td>\n",
       "      <td>Formosa 1018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  content  label  \\\n",
       "64160                                          早餐人太多排隊排很久      0   \n",
       "19452                                                   無      0   \n",
       "266740                        很舒適的旅館和服務，下次會再來...早餐好吃極了...      1   \n",
       "126829                                              早餐有點差      0   \n",
       "129917  地點有點尷尬，周遭的華陰街晚上基本上都大門深鎖沒有宵夜，因此不如南陽街；也由於鄰近店家很早就...      0   \n",
       "122265                                               沒有窗戶      0   \n",
       "276984                                  房間乾淨舒適，cp值高！值得推薦！      1   \n",
       "91502                        房間打掃不完整 桌上迎賓糖果裡居然有已經吃過的糖果紙包裝      0   \n",
       "240231                         床很舒服 床間也很棒行李部的服務員也很好 很樂於助人      1   \n",
       "258390  早餐還有吐司、貝果，水果切盤，飲料咖啡無限喝，床很舒適，吧檯員工新來的很風趣、很健談、重點會...      1   \n",
       "\n",
       "            hotel_name  \n",
       "64160        娜路彎大酒店329  \n",
       "19452        花蓮鳥窩青年旅舍3  \n",
       "266740      東驛商旅 西寧館11  \n",
       "126829       雙星大飯店1508  \n",
       "129917  龍蝦先生的秘密巢穴設計青旅7  \n",
       "122265      台北大亞帝國H709  \n",
       "276984         新京站民宿63  \n",
       "91502         昭盛52行館18  \n",
       "240231        義大皇家酒店55  \n",
       "258390    Formosa 1018  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('reviews.csv')\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-chinese'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_txt = df.content[150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntokens = tokenizer.tokenize(sample_txt)\\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\\n\\nprint(f' Sentence: {sample_txt}')\\nprint(f'   Tokens: {tokens}')\\nprint(f'Token IDs: {token_ids}')\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "tokens = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(f' Sentence: {sample_txt}')\n",
    "print(f'   Tokens: {tokens}')\n",
    "print(f'Token IDs: {token_ids}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nencoding = tokenizer.encode_plus(\\n  sample_txt,\\n  max_length=512,\\n  truncation=True,\\n  add_special_tokens=True, \\n  return_token_type_ids=False,\\n  pad_to_max_length=True,\\n  return_attention_mask=True,\\n  return_tensors='pt',  \\n)\\n\\nencoding.keys()\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "encoding = tokenizer.encode_plus(\n",
    "  sample_txt,\n",
    "  max_length=512,\n",
    "  truncation=True,\n",
    "  add_special_tokens=True, \n",
    "  return_token_type_ids=False,\n",
    "  pad_to_max_length=True,\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',  \n",
    ")\n",
    "\n",
    "encoding.keys()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(encoding['input_ids'][0]))\n",
    "# encoding['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(encoding['attention_mask'][0]))\n",
    "# encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPReviewDataset(Dataset):\n",
    "\n",
    "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "    self.reviews = reviews\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.reviews)\n",
    "  \n",
    "  def __getitem__(self, item):\n",
    "    review = str(self.reviews[item])\n",
    "    target = self.targets[item]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      review,\n",
    "      add_special_tokens=True,\n",
    "      truncation=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'review_text': review,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76802     0\n",
       "174405    1\n",
       "273718    1\n",
       "262133    1\n",
       "116991    0\n",
       "242545    1\n",
       "131065    0\n",
       "266830    1\n",
       "334871    1\n",
       "6681      0\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.label.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = GPReviewDataset(\n",
    "    reviews=df.content.to_numpy(),\n",
    "    targets=df.label.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes):\n",
    "    super(SentimentClassifier, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    _, pooled_output = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "    output = self.drop(pooled_output)\n",
    "    return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 2 GPUs!\n"
     ]
    }
   ],
   "source": [
    "class_names = ['negative', 'positive']\n",
    "model = SentimentClassifier(len(class_names))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 4\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "  model, \n",
    "  data_loader, \n",
    "  loss_fn, \n",
    "  optimizer, \n",
    "  device, \n",
    "  scheduler, \n",
    "  n_examples\n",
    "):\n",
    "  model = model.train()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  \n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "\n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "      loss = loss_fn(outputs, targets)\n",
    "\n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "  print('-' * 10)\n",
    "\n",
    "  train_acc, train_loss = train_epoch(\n",
    "    model,\n",
    "    train_data_loader,    \n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    device, \n",
    "    scheduler, \n",
    "    len(df_train)\n",
    "  )\n",
    "\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "  val_acc, val_loss = eval_model(\n",
    "    model,\n",
    "    val_data_loader,\n",
    "    loss_fn, \n",
    "    device, \n",
    "    len(df_val)\n",
    "  )\n",
    "\n",
    "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "  print()\n",
    "\n",
    "  history['train_acc'].append(train_acc)\n",
    "  history['train_loss'].append(train_loss)\n",
    "  history['val_acc'].append(val_acc)\n",
    "  history['val_loss'].append(val_loss)\n",
    "\n",
    "  if val_acc > best_accuracy:\n",
    "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "    best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp] *",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
